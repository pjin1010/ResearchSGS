{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9c9ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterjin/opt/anaconda3/lib/python3.9/site-packages/xarray/backends/cfgrib_.py:27: UserWarning: Failed to load cfgrib - most likely there is a problem accessing the ecCodes library. Try `import cfgrib` to get the full error message\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pyqg\n",
    "import json\n",
    "\n",
    "from pyqg.diagnostic_tools import calc_ispec as _calc_ispec\n",
    "%run coarsening_ops.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "calc_ispec = lambda *args, **kwargs: _calc_ispec(*args, averaging=False, truncate=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48d51c",
   "metadata": {},
   "source": [
    "### ––– Forcing Data ––– \n",
    "---\n",
    "###### Inputs:\n",
    "- ```nx_hires```: number of grid points in high resolution (Spatial)\n",
    "- ```nx_lores```: number of grid points in low resolution (Spatial)\n",
    "- ```dt```: timestep (Temporal)\n",
    "- ```tmax```: simulation time (Temporal)\n",
    "- ```tavestart```: time at which we begin averaging diagnostics (Temporal)\n",
    "\n",
    "\n",
    "###### Outputs:\n",
    "- A list of ```forcing datasets``` containing filtered and coarse-grained high resolution snapshots along with diagnostics and subgrid forcing variables. \n",
    "- Each element of the list corresponds with a particular filtering and coarse-graining method.\n",
    "---\n",
    "Notes: \n",
    "- Generally speaking, we can play around with the temporal side if we want larger/smaller datasets. Quasisteady state takes ~5 years. \n",
    "- The spatial side will likely remain the same as eddy/jet configurations rely on the Rossby deformation radius, which is defined by L (real space grid size) and nx (number of grid points). Since L is more or less held constant, nx should also.\n",
    "---\n",
    "TO DO:\n",
    "- Generalize function to take in arbitrary amounts of filtering and coarse-graining methods\n",
    "    - add input variable of such operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed4a6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forcing_data(nx_hires = 256, nx_lores = 64, dt = 3600.0, tmax = 311040000.0, tavestart = 155520000.0):\n",
    "    # ——— FORCING DATASETS ———\n",
    "    # Create datasets with:\n",
    "    # 1. Snapshots of filtered and coarse-grained high resolution simulations at every timestep\n",
    "    # 2. Associated subgrid forcings\n",
    "    # 3. Diagnostics evaluated at the end (most recent, should already be time averaged)\n",
    "\n",
    "    snapshots_hr = [] # high resolution snapshots\n",
    "    forcing_1 = [] \n",
    "    forcing_2 = []\n",
    "    forcing_3 = []\n",
    "    \n",
    "    base_kwargs = {'dt': dt, 'tmax': tmax, 'tavestart': tavestart}\n",
    "    high_res = pyqg.QGModel(nx=nx_hires, **base_kwargs)\n",
    "\n",
    "    # note: since each count doesn't update (reinitialization at every step):\n",
    "    count = [0,0,0] # 1. to iterate diagnostics\n",
    "    diag_temp = [] # 2. to save diagnostics of previous iteration\n",
    "\n",
    "    while high_res.t < high_res.tmax:\n",
    "        if high_res.tc % 1000 == 0: # every 1000 hours\n",
    "            snapshots_hr.append(high_res.to_dataset().copy(deep=True)) # note: this is the only thing in snapshots_hr\n",
    "\n",
    "            op1 = Operator1(high_res, nx_lores)\n",
    "            op2 = Operator2(high_res, nx_lores) \n",
    "            op3 = Operator3(high_res, nx_lores)\n",
    "            ops = [op1, op2, op3]\n",
    "\n",
    "            fcg_lowres = [] # temporary array to store filtered & coarse-grained high resolution and its variables\n",
    "\n",
    "            # 1. loop to store variables over time since the operators re-initialize low res simulation at every call\n",
    "            for i, op in enumerate(ops):\n",
    "\n",
    "                # ————————————————— DIAGNOSTICS —————————————————\n",
    "                op.m2._initialize_diagnostics(diagnostics_list='all')\n",
    "\n",
    "                if len(diag_temp) == 3: # make sure we have diagnostics in this array (i.e. skip first loop)\n",
    "                    op.m2.diagnostics = diag_temp[i] # initialize our new m2 with previous iteration's diagnostics\n",
    "\n",
    "                # See: _increment_diagnostics() \n",
    "                # Link: https://github.com/pyqg/pyqg/blob/8a792d4d4d36580af025b002417be60afd6a991a/pyqg/model.py#L770\n",
    "                if (high_res.t>=high_res.dt) and (high_res.t>=high_res.tavestart) and (high_res.tc%high_res.taveints==0):\n",
    "                    for d in op.m2.diagnostics:\n",
    "                        if op.m2.diagnostics[d]['active']:\n",
    "                            op.m2.diagnostics[d]['count'] = count[i]\n",
    "                    op.m2._increment_diagnostics()\n",
    "                    count[i]+=1\n",
    "\n",
    "                # 1. save diagnostics to initialize m2 diagnostics in next iteration   \n",
    "                if len(diag_temp) != 3:\n",
    "                    diag_temp.append(op.m2.diagnostics)\n",
    "                diag_temp[i] = op.m2.diagnostics \n",
    "\n",
    "                # ———————————————————— TIME —————————————————————\n",
    "                # 1. transform models into datasets\n",
    "                # 2. set the time variable of the coarsened model\n",
    "                temp = op.m2.to_dataset().copy(deep=True)\n",
    "                temp['time'] = [high_res.t]\n",
    "                fcg_lowres.append(temp)\n",
    "\n",
    "                # —————————————————— FORCINGS ——————————————————\n",
    "                # 1. add forcing terms to dataset\n",
    "                for var in ['q','u','v']:\n",
    "                    fcg_lowres[i][var+'_subgrid_forcing'] = (['lev','y','x'], op.subgrid_forcing(var))\n",
    "\n",
    "                uq_flux, vq_flux = op.subgrid_fluxes('q')\n",
    "                fcg_lowres[i]['uq_subgrid_flux'] = (['lev','y','x'], uq_flux)\n",
    "                fcg_lowres[i]['vq_subgrid_flux'] = (['lev','y','x'], vq_flux)\n",
    "\n",
    "                uu_flux, uv_flux = op.subgrid_fluxes('u')\n",
    "                vv_flux, vu_flux = op.subgrid_fluxes('v')\n",
    "                fcg_lowres[i]['uu_subgrid_flux'] = (['lev','y','x'], uu_flux)\n",
    "                fcg_lowres[i]['vv_subgrid_flux'] = (['lev','y','x'], vv_flux)\n",
    "                fcg_lowres[i]['uv_subgrid_flux'] = (['lev','y','x'], uv_flux)\n",
    "\n",
    "                dqdt_bar = op.coarsen(op.m1.dqhdt)\n",
    "                dqbar_dt = op.to_real(op.m2.dqhdt)\n",
    "                fcg_lowres[i]['dqdt_bar'] = (['lev','y','x'], dqdt_bar)\n",
    "                fcg_lowres[i]['dqbar_dt'] = (['lev','y','x'], dqbar_dt)\n",
    "\n",
    "            forcing_1.append(fcg_lowres[0])\n",
    "            forcing_2.append(fcg_lowres[1])\n",
    "            forcing_3.append(fcg_lowres[2])\n",
    "        high_res._step_forward()\n",
    "    \n",
    "    forcing = [forcing_1, forcing_2, forcing_3]\n",
    "\n",
    "    # 1. create dataset by joining datasets along the temporal axis \n",
    "    ds_forcing = []\n",
    "    for f in forcing:\n",
    "        ds_forcing.append(xr.concat(f, dim='time'))\n",
    "    \n",
    "    # 2. clean up/tie up loose ends\n",
    "    for i,f in enumerate(forcing):\n",
    "        # 2a. add back diagnostics that may have been dropped by the above since some lack a time coordinate\n",
    "        for k,v in f[-1].variables.items():\n",
    "            if k not in ds_forcing[i]:\n",
    "                ds_forcing[i][k] = v.isel(time=-1)\n",
    "        \n",
    "        # 2b. drop complex vars since they cannot be saved\n",
    "        complex_vars = [k for k,v in ds_forcing[i].variables.items() if np.iscomplexobj(v)]\n",
    "        ds_forcing[i] = ds_forcing[i].drop_vars(complex_vars)\n",
    "        ds_forcing[i] = ds_forcing[i].drop_vars('dqdt')\n",
    "\n",
    "        # 2c. add missing attributes  \n",
    "        ds_forcing[i].attrs['hires'] = nx_hires \n",
    "        ds_forcing[i].attrs['lores'] = nx_lores\n",
    "        base_kwargs['nx'] = nx_lores\n",
    "        ds_forcing[i].attrs['pyqg_params'] = base_kwargs   \n",
    "        \n",
    "    return ds_forcing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
