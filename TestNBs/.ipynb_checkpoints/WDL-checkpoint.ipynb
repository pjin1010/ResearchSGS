{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b1b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By Zhewen Hou\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.distributions.normal import Normal\n",
    "from scipy.stats import norm\n",
    "from sklearn import *\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071818d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy implementation\n",
    "# def dgmm1d(x, mu, sigma, pi):\n",
    "#    pdf_gmm = np.sum([pi[k] * norm.pdf(x, loc=mu[k], scale=sigma[k]) for k in range(len(mu))], axis=0)\n",
    "#    return pdf_gmm\n",
    "\n",
    "# density of 1D Gaussian Mixture\n",
    "def dgmm1d(x: Tensor, mu: Tensor, sigma: Tensor, pi: Tensor) -> Tensor:\n",
    "    pdf_gmm = torch.sum(torch.stack([pi[k] * Normal(mu[k], sigma[k]).log_prob(x).exp() for k in range(len(mu))]), dim=0)\n",
    "    return pdf_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cf6686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy implementation\n",
    "# def pgmm1d(x, mu, sigma, pi):\n",
    "#    cdf_gmm = np.sum([pi[k] * norm.cdf(x, loc=mu[k], scale=sigma[k]) for k in range(len(mu))], axis=0)\n",
    "#    return cdf_gmm\n",
    "\n",
    "# cdf of 1D Gaussian Mixture\n",
    "def pgmm1d(x: Tensor, mu: Tensor, sigma: Tensor, pi: Tensor) -> Tensor:\n",
    "    cdf_gmm = torch.sum(torch.stack([pi[k] * Normal(mu[k], sigma[k]).cdf(x) for k in range(len(mu))]), dim=0)\n",
    "    return cdf_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be32138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy implementation: quantile of 1D Gaussian Mixture\n",
    "#def qgmm1d(q, mu, sigma, pi):\n",
    "#    ppf_full = np.array([norm.ppf(q, loc=mu[k], scale=sigma[k]) for k in range(len(mu))]).flatten()\n",
    "#    ppf_full.sort()\n",
    "#    cdf_gmm = np.sum([pi[k] * norm.cdf(ppf_full, loc=mu[k], scale=sigma[k]) for k in range(len(mu))], axis=0)\n",
    "#    ## 1D linear interpolation\n",
    "#    ppf_gmm = np.interp(q, cdf_gmm, ppf_full)\n",
    "#    return ppf_gmm\n",
    "\n",
    "# implementation of np.interp with pytorch\n",
    "def interp(x: Tensor, xp: Tensor, fp: Tensor) -> Tensor:\n",
    "    m = (fp[:, 1:] - fp[:, :-1]) / (xp[:, 1:] - xp[:, :-1])\n",
    "    b = fp[:, :-1] - (m * xp[:, :-1])\n",
    "\n",
    "    indicies = torch.sum(torch.ge(x[:, :, None], xp[:, None, :]), dim=2) - 1\n",
    "    indicies = torch.clamp(indicies, 0, len(m[0]) - 1)\n",
    "\n",
    "    return torch.gather(m, 1, indicies) * x + torch.gather(b, 1, indicies)\n",
    "\n",
    "\n",
    "# quantile of 1D Gaussian Mixture\n",
    "def qgmm1d(q: Tensor, mu: Tensor, sigma: Tensor, pi: Tensor) -> Tensor:\n",
    "    #print((Normal(mu, sigma).icdf(q.view(q.size(0), 1, 1).expand(q.size(0), mu.size(0), mu.size(1)))).shape)\n",
    "    ppf_full = torch.permute(Normal(mu, sigma).icdf(q.view(q.size(0), 1, 1).expand(q.size(0), mu.size(0), mu.size(1))),\n",
    "                             (1, 0, 2)).flatten(1)\n",
    "    # ppf_full = torch.permute(Normal(mu, sigma).icdf(q[:, None, None].expand(q.size(0), mu.size(0), mu.size(1))),\n",
    "    #                         (1, 0, 2)).flatten(1)\n",
    "    ppf_full= torch.sort(ppf_full, dim=1)[0]\n",
    "    cdf_gmm = torch.sum(pi[:, None] * Normal(mu[:, None], sigma[:, None]).cdf(ppf_full[:, :, None]), dim=2)\n",
    "    ppf_gmm = interp(q.expand(mu.size(0), -1), cdf_gmm, ppf_full)\n",
    "    return ppf_gmm.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5f0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dWasserstein(Y: Tensor, mu: Tensor, sigma: Tensor, pi: Tensor, q: Tensor, p: int = 2) -> Tensor:\n",
    "    ppf_gmm = qgmm1d(q, mu, sigma, pi)\n",
    "    #print(ppf_gmm.shape)\n",
    "    #print(Y.shape)\n",
    "    Wp = torch.mean((ppf_gmm - Y) ** p, dim=1) ** (1 / p)\n",
    "    return Wp\n",
    "\n",
    "\n",
    "# main functions for fitting Wasserstein mixture regression\n",
    "def WDL(Y, q_vec, mu, sd, pi):\n",
    "    \"\"\"\n",
    "    n: number of observations\n",
    "    m: number of percentiles \n",
    "    k: number of components in the GMM \n",
    "    :param Y: n x m\n",
    "    :param q_vec: m\n",
    "    :param mu: n x k, mean of the components in the GMM\n",
    "    :param sd: n x k, standard deviation of the components in the GMM\n",
    "    :param pi: n x k, mixing proportion of the components in the GMM\n",
    "    :return: mean distance between the empirical quantile function and the quantile function of the GMM\n",
    "    \"\"\"\n",
    "    # calculate the mean of the Wasserstein distance for each distribution of different X\n",
    "    w2 = torch.mean(batch_dWasserstein(Y, mu, sd, pi, q_vec) ** 2)\n",
    "    return w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d7da6",
   "metadata": {},
   "source": [
    "#### Testing Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f739f43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n",
      "[[-3.09883251 -2.86643922 -2.51388866 ...  4.31112254  4.3739572\n",
      "   5.20220006]\n",
      " [-2.91661178 -2.65928491 -2.44640414 ...  4.53591171  4.56868362\n",
      "   4.62578348]\n",
      " [-2.4340326  -2.40057418 -2.28007935 ...  5.02492275  5.16380261\n",
      "   5.78538448]\n",
      " ...\n",
      " [-2.46941656 -2.20831643 -2.10139391 ...  5.88877085  6.20330087\n",
      "   6.4078245 ]\n",
      " [-2.84557891 -2.6724476  -2.48777096 ...  4.07628841  4.22882558\n",
      "   4.26913273]\n",
      " [-3.17172267 -3.08769588 -3.08434324 ...  4.32627668  4.42752034\n",
      "   4.526383  ]]\n",
      "q_vec tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800, 0.0900,\n",
      "        0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700, 0.1800,\n",
      "        0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600, 0.2700,\n",
      "        0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500, 0.3600,\n",
      "        0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400, 0.4500,\n",
      "        0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300, 0.5400,\n",
      "        0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200, 0.6300,\n",
      "        0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100, 0.7200,\n",
      "        0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000, 0.8100,\n",
      "        0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900, 0.9000,\n",
      "        0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800, 0.9900],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# set parameters\n",
    "SEED = 234234\n",
    "K = 2 # number of components in the GMM\n",
    "n_dist = 300 ## number of distributions\n",
    "omega = 0.1 # default 0.1\n",
    "n_sample = 300\n",
    "\n",
    "# simulate the data\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "X = np.random.uniform(size=(n_dist, 3)) * 2 - 1 # generate X\n",
    "Y = np.zeros((n_dist, n_sample)) # generate Y\n",
    "\n",
    "\n",
    "## simulate Y\n",
    "for i in range(n_dist):\n",
    "    mu_1 = X[i, 0]\n",
    "    mu_2 = 2 * X[i, 1]**2 + 2\n",
    "    mu_true = [mu_1, mu_2]\n",
    "    #print('mu:', mu_true)\n",
    "    sig_1 = np.abs(X[i, 1]) + 0.5\n",
    "    sig_2 = np.abs(X[i, 0]) + 0.5\n",
    "    sig_true = [sig_1, sig_2]\n",
    "    #print('sig:', sig_true)\n",
    "    pi_1 = 1 / (1 + np.exp(X[i, 2]))\n",
    "    pi_true = [pi_1, 1-pi_1]\n",
    "    #print('pi:', pi_true)\n",
    "    ## simulate noise\n",
    "    eps_noise = np.random.normal(loc=0, scale=omega, size=1)\n",
    "    ## simulate responses\n",
    "    var_gaussian = np.array([np.random.normal(loc=mu_true[k]+eps_noise,\n",
    "                                              scale=sig_true[k],\n",
    "                                              size=n_sample) for k in range(K)]).T\n",
    "    var_mult = np.random.choice(range(K), size=n_sample, replace=True, p=pi_true)\n",
    "    var_mult = np.eye(K)[var_mult]\n",
    "    var_GMM = np.sum(var_mult * var_gaussian, axis=1)\n",
    "    Y[i] = np.sort(var_GMM)\n",
    "    \n",
    "print(Y.shape)\n",
    "print(Y)\n",
    "\n",
    "# prepare for model fitting\n",
    "K_mix = 2\n",
    "n_iter = 1000\n",
    "lr = 1e-1\n",
    "v_lr = np.array([1] + [lr] * n_iter)\n",
    "n_dist = Y.shape[0]\n",
    "n_levs = 100\n",
    "q_vec = torch.arange(1, n_levs, dtype=torch.float64) / n_levs # quantile levels\n",
    "print('q_vec', q_vec)\n",
    "## transform Y\n",
    "Q_mat = np.array([np.quantile(Y[i], q_vec) for i in range(n_dist)]) # calculate the empirical quantile function\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1be7060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.3608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.0676, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1523, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.4872, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1873, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5498, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.3114, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.0874, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(12.4420, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.5198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3544, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.4462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.3674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(16.3623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(7.2067, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.1484, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5112, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.7047, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(7.4095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(9.3400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.2808, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1035, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.2472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.9171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.4767, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.8100, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.1226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.2997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(7.8719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.0793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.1619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0146, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.1283, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.7620, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.3084, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.1680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0325, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.6671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(10.0313, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8868, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8800, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.2519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.1892, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.9665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0101, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5557, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.9314, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.2226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.3710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.5030, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(1.7162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8779, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(1.7970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.4300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.6133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.9027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.9573, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.0200, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(9.3484, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(7.4507, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.0173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.8513, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(10.0262, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(7.0725, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.2623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.1418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(9.5841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.0724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.3578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.5845, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.5698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(11.0337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(9.5888, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.4720, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(5.7526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(9.0107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(3.4680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(8.5747, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.8503, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(4.2801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.2511, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(2.6411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor(6.6719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "4.048244953155518\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# step 1. train-val split\n",
    "n_train = int(0.8 * n_dist)\n",
    "loc_train = np.random.choice(n_dist, n_train, replace=False)\n",
    "loc_val = np.setdiff1d(np.arange(n_dist), loc_train)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X[loc_train])\n",
    "# generate true parameters\n",
    "mu_true = torch.cat((X_train[:, 0].unsqueeze(dim=1), (2 * X_train[:, 1] ** 2 + 2).unsqueeze(dim=1)), dim=1)\n",
    "sd_true = torch.cat(\n",
    "    (torch.abs(X_train[:, 1]).unsqueeze(dim=1) + 0.5, torch.abs(X_train[:, 0]).unsqueeze(dim=1) + 0.5), dim=1)\n",
    "alpha_true = torch.cat((torch.zeros_like(X_train[:, 2].unsqueeze(dim=1)), X_train[:, 2].unsqueeze(dim=1)), dim=1)\n",
    "pi_true = torch.softmax(alpha_true, dim=1)\n",
    "\n",
    "# generate features\n",
    "X_train = torch.cat((X_train, torch.abs(X_train), X_train ** 2), 1)\n",
    "y_train = torch.tensor(Q_mat[loc_train])\n",
    "X_val = torch.tensor(X[loc_val])\n",
    "X_val = torch.cat((X_val, torch.abs(X_val), X_val ** 2), 1)\n",
    "y_val = torch.tensor(Q_mat[loc_val])\n",
    "\n",
    "\n",
    "# test the speed of the loss function\n",
    "start = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    alpha_matrix = torch.tensor(np.random.randn(9, 2), requires_grad=True)\n",
    "    mu_matrix = torch.tensor(np.random.randn(9, 2), requires_grad=True)\n",
    "    sd_matrix = torch.tensor(np.random.randn(9, 2), requires_grad=True)\n",
    "\n",
    "    alpha = torch.matmul(X_train, alpha_matrix)\n",
    "    pi = torch.softmax(alpha, dim=1)\n",
    "\n",
    "    mu = torch.matmul(X_train, mu_matrix)\n",
    "    \n",
    "    sd = torch.abs(torch.matmul(X_train, sd_matrix))\n",
    "\n",
    "    loss = WDL(y_train, q_vec, mu, sd, pi)\n",
    "\n",
    "    print(loss)\n",
    "print(time.time() - start)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8383b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.680827736958281\n",
      "1 1.4953733142883936\n",
      "2 0.8903266381366824\n",
      "3 0.7522665855918405\n",
      "4 0.6710365909077746\n",
      "5 0.6144841417018095\n",
      "6 0.5742080114865169\n",
      "7 0.5443134943569208\n",
      "8 0.5219079492396036\n",
      "9 0.5045083459416578\n",
      "10 0.490555600545359\n",
      "11 0.47903784840924074\n",
      "12 0.4693277394143237\n",
      "13 0.4608812965751459\n",
      "14 0.453397342155681\n",
      "15 0.44663065216368936\n",
      "16 0.4404143981473953\n",
      "17 0.4346335542102026\n",
      "18 0.42921066264910884\n",
      "19 0.424084975169027\n",
      "20 0.41921745318801584\n",
      "21 0.41458188412818564\n",
      "22 0.41014186086025883\n",
      "23 0.405878811238565\n",
      "24 0.40177200164886845\n",
      "25 0.39781512107280065\n",
      "26 0.3939923916591563\n",
      "27 0.39030338083712146\n",
      "28 0.38673767590730523\n",
      "29 0.38329830855231933\n",
      "30 0.3799810842033363\n",
      "31 0.3767588206190716\n",
      "32 0.37363722823770573\n",
      "33 0.3706204708419906\n",
      "34 0.3677066754209915\n",
      "35 0.36489828297820875\n",
      "36 0.3621896742750117\n",
      "37 0.3595685966125013\n",
      "38 0.3570516113984413\n",
      "39 0.35464455808546586\n",
      "40 0.35234502680068236\n",
      "41 0.3501622599364519\n",
      "42 0.34809768744946507\n",
      "43 0.34618489038970174\n",
      "44 0.34441622281541406\n",
      "45 0.3428365284541093\n",
      "46 0.34147280125706014\n",
      "47 0.34038975625277534\n",
      "48 0.33961756803100657\n",
      "49 0.339264074945775\n",
      "50 0.33937783254580084\n",
      "51 0.3400744368486927\n",
      "52 0.34150695500120365\n",
      "53 0.34397240600225854\n",
      "54 0.3475932958796861\n",
      "55 0.3528063569082972\n",
      "56 0.35978819470498513\n",
      "57 0.3693281618798594\n",
      "58 0.3815235985742806\n",
      "59 0.39794017478573157\n",
      "60 0.4183204710180647\n",
      "61 0.44568805803428846\n",
      "62 0.47901553254458373\n",
      "63 0.5235980068136639\n",
      "64 0.5750692482695704\n",
      "65 0.644032932472481\n",
      "66 0.7223271674408853\n",
      "67 0.8220670645416168\n",
      "68 0.831829963856581\n",
      "69 0.932858389136873\n",
      "70 0.8478858552051435\n",
      "71 0.9238932043295371\n",
      "72 0.798333991004583\n",
      "73 0.8439415980981517\n",
      "74 0.7285404891197221\n",
      "75 0.7511984710078257\n",
      "76 0.6311679056446821\n",
      "77 0.640029819740217\n",
      "78 0.5776841182247557\n",
      "79 0.581060782024868\n",
      "80 0.5238774100966406\n",
      "81 0.523796512617075\n",
      "82 0.5167272124242337\n",
      "83 0.5157401752322549\n",
      "84 0.506237726747828\n",
      "85 0.5042727989485605\n",
      "86 0.49454483370205876\n",
      "87 0.49194929536919524\n",
      "88 0.48238130002829693\n",
      "89 0.4794981106230857\n",
      "90 0.46944771451408523\n",
      "91 0.4664159926809977\n",
      "92 0.4576863247797503\n",
      "93 0.4546248169175748\n",
      "94 0.4465061153138423\n",
      "95 0.44345194919406655\n",
      "96 0.4357373144553032\n",
      "97 0.4329772625948152\n",
      "98 0.4259195125525636\n",
      "99 0.4234316579599293\n",
      "100 0.41710171727348117\n",
      "101 0.4150117186778223\n",
      "102 0.40952584739676084\n",
      "103 0.4077198732366601\n",
      "104 0.4026234658904561\n",
      "105 0.4007289183690007\n",
      "106 0.3961204347990709\n",
      "107 0.3944824313071262\n",
      "108 0.39042092621192287\n",
      "109 0.3889512005193386\n",
      "110 0.38483025203091675\n",
      "111 0.3836100353776558\n",
      "112 0.37973336212127157\n",
      "113 0.37843685006816535\n",
      "114 0.3748992596210172\n",
      "115 0.3735903751792165\n",
      "116 0.370400224957094\n",
      "117 0.36923938075167706\n",
      "118 0.36661113473778323\n",
      "119 0.36506580734112265\n",
      "120 0.3616413566169854\n",
      "121 0.36003811177158046\n",
      "122 0.35709450206254517\n",
      "123 0.35550943503160837\n",
      "124 0.3530963210187799\n",
      "125 0.3512672103884313\n",
      "126 0.34836092744420993\n",
      "127 0.3465442219017286\n",
      "128 0.3436738775177342\n",
      "129 0.3417326253448619\n",
      "130 0.33945523552931856\n",
      "131 0.3374315157829548\n",
      "132 0.3354393054552628\n",
      "133 0.33337927096121167\n",
      "134 0.3312511873638061\n",
      "135 0.32907602991038404\n",
      "136 0.3267458594836745\n",
      "137 0.3245110186112888\n",
      "138 0.32227797756889914\n",
      "139 0.31993385315589523\n",
      "140 0.31773266491521496\n",
      "141 0.31549180995771753\n",
      "142 0.313431734843706\n",
      "143 0.3114280015359101\n",
      "144 0.30964675666458114\n",
      "145 0.3075244674181444\n",
      "146 0.3057559271211406\n",
      "147 0.3038156763668075\n",
      "148 0.3022274232787384\n",
      "149 0.3003043277450509\n",
      "150 0.298905480205301\n",
      "151 0.29687268769084313\n",
      "152 0.29535805497397066\n",
      "153 0.2934909265551173\n",
      "154 0.2919532664613398\n",
      "155 0.29010046468486345\n",
      "156 0.28914249395780284\n",
      "157 0.28727200842607536\n",
      "158 0.2858838320069046\n",
      "159 0.2841291936405777\n",
      "160 0.28278069505512204\n",
      "161 0.28118172499533645\n",
      "162 0.280238779586441\n",
      "163 0.27845555678956124\n",
      "164 0.2773212775729127\n",
      "165 0.2755786966152731\n",
      "166 0.2743242633244435\n",
      "167 0.2726761465757173\n",
      "168 0.27185649099664766\n",
      "169 0.27005099259176707\n",
      "170 0.2690111331779716\n",
      "171 0.24084372038041751\n",
      "172 0.22830423345021741\n",
      "173 0.22244858758489866\n",
      "174 0.21963549339754523\n",
      "175 0.2182643297900782\n",
      "176 0.21750149559316204\n",
      "177 0.21702351916736712\n",
      "178 0.21666672986079696\n",
      "179 0.2163767482399028\n",
      "180 0.21611964515808288\n",
      "181 0.21588080308631666\n",
      "182 0.21565208906419836\n",
      "183 0.21543225534276686\n",
      "184 0.21521629567464054\n",
      "185 0.21500393090105385\n",
      "186 0.21479544067236306\n",
      "187 0.21459027427372637\n",
      "188 0.21438926305290157\n",
      "189 0.21419265981089158\n",
      "190 0.21400011870981142\n",
      "191 0.21381115611621934\n",
      "192 0.21362531894667583\n",
      "193 0.21344269469472196\n",
      "194 0.21326343649777674\n",
      "195 0.21308664519148754\n",
      "196 0.2129122393178424\n",
      "197 0.21271635045905696\n",
      "198 0.2125342309327899\n",
      "199 0.21235760187473113\n",
      "200 0.21218604741080468\n",
      "201 0.21201685212241664\n",
      "202 0.21184850545790426\n",
      "203 0.21168271915854292\n",
      "204 0.2115195659369412\n",
      "205 0.21135912297319984\n",
      "206 0.21120057709156567\n",
      "207 0.21104416429516357\n",
      "208 0.2108900130384012\n",
      "209 0.21073809836276058\n",
      "210 0.21058712305193114\n",
      "211 0.21043811339412055\n",
      "212 0.21029127046330276\n",
      "213 0.2101462959061218\n",
      "214 0.21000215831237315\n",
      "215 0.20986003006645718\n",
      "216 0.209694142668929\n",
      "217 0.20954078542703397\n",
      "218 0.2093918199232584\n",
      "219 0.20924348924070704\n",
      "220 0.20909782156821083\n",
      "221 0.20895580633210803\n",
      "222 0.20881547551648444\n",
      "223 0.20867695408194833\n",
      "224 0.20853986284976822\n",
      "225 0.20840415328797043\n",
      "226 0.20826937575099796\n",
      "227 0.20813621690986667\n",
      "228 0.2080045710707671\n",
      "229 0.20787444948944261\n",
      "230 0.20774500636872553\n",
      "231 0.20761660316179384\n",
      "232 0.2074879016769314\n",
      "233 0.2073359570152294\n",
      "234 0.2071956145105552\n",
      "235 0.2070585462046175\n",
      "236 0.20692165169541904\n",
      "237 0.2067882823449653\n",
      "238 0.20665716279192753\n",
      "239 0.2065271087526543\n",
      "240 0.20639930754903613\n",
      "241 0.20627329614582485\n",
      "242 0.20614831994767283\n",
      "243 0.20602427937234075\n",
      "244 0.20590104508050858\n",
      "245 0.20577889183561\n",
      "246 0.20565782045826816\n",
      "247 0.20551420534839443\n",
      "248 0.2053820939450866\n",
      "249 0.2052525496707081\n",
      "250 0.205121669796659\n",
      "251 0.20499399297092696\n",
      "252 0.20486889233197772\n",
      "253 0.2047447232948273\n",
      "254 0.20462160768458423\n",
      "255 0.20449900900666582\n",
      "256 0.20437786345925094\n",
      "257 0.20425741915869983\n",
      "258 0.2041379442693542\n",
      "259 0.20401714150395397\n",
      "260 0.20387612168283178\n",
      "261 0.20374642343328248\n",
      "262 0.20362016645906164\n",
      "263 0.20349627156465722\n",
      "264 0.20336955001157608\n",
      "265 0.20324532012122595\n",
      "266 0.20312302927593837\n",
      "267 0.20300106211840202\n",
      "268 0.20287958547324986\n",
      "269 0.20275873302296282\n",
      "270 0.20263857468158383\n",
      "271 0.20251899992332986\n",
      "272 0.20239991362193432\n",
      "273 0.20227948194062073\n",
      "274 0.20214017157265757\n",
      "275 0.20201127387681875\n",
      "276 0.20188411300799344\n",
      "277 0.20175961010425278\n",
      "278 0.2016354898203818\n",
      "279 0.2015082481875476\n",
      "280 0.20138355211452985\n",
      "281 0.2012598262352102\n",
      "282 0.20113598888032028\n",
      "283 0.2010127987384763\n",
      "284 0.20089026659876694\n",
      "285 0.2007676025583829\n",
      "286 0.20064470220106664\n",
      "287 0.200501353132554\n",
      "288 0.20036677584126075\n",
      "289 0.20023455534060045\n",
      "290 0.20010482918257733\n",
      "291 0.19997594290700013\n",
      "292 0.19984731455688656\n",
      "293 0.19971798876718427\n",
      "294 0.1995852261351063\n",
      "295 0.19945368458886517\n",
      "296 0.19932265688902734\n",
      "297 0.19919161063867247\n",
      "298 0.19905985405775053\n",
      "299 0.19892638340544344\n",
      "300 0.1987758813069391\n",
      "301 0.1986342082906886\n",
      "302 0.19849392126105203\n",
      "303 0.1983554246687995\n",
      "304 0.1982170819592258\n",
      "305 0.1980782977090264\n",
      "306 0.19793935109976277\n",
      "307 0.19780048554290244\n",
      "308 0.19765729739780624\n",
      "309 0.19751487210938465\n",
      "310 0.1973728335240576\n",
      "311 0.19723068358277818\n",
      "312 0.19708459972748535\n",
      "313 0.19691990116371053\n",
      "314 0.19676389517945178\n",
      "315 0.1966098779788594\n",
      "316 0.19645793145006044\n",
      "317 0.19630528202037534\n",
      "318 0.19615218184328262\n",
      "319 0.19599939773821332\n",
      "320 0.19584606446655325\n",
      "321 0.1956925252718771\n",
      "322 0.19553792282135427\n",
      "323 0.19538150936115997\n",
      "324 0.19521944265408459\n",
      "325 0.19503395354727432\n",
      "326 0.19485778377599897\n",
      "327 0.19468433905089247\n",
      "328 0.19451163506348496\n",
      "329 0.194337215999208\n",
      "330 0.19416209484080243\n",
      "331 0.1939856456837608\n",
      "332 0.1938088216843452\n",
      "333 0.19363098137049367\n",
      "334 0.1934519008337969\n",
      "335 0.19327148738590297\n",
      "336 0.19308790671416434\n",
      "337 0.19288001266626773\n",
      "338 0.19267430420752935\n",
      "339 0.19247223825498452\n",
      "340 0.19227105018527638\n",
      "341 0.1921017715705908\n",
      "342 0.19193194355204682\n",
      "343 0.19176164214329205\n",
      "344 0.19159032682875624\n",
      "345 0.19141802000816313\n",
      "346 0.19124461119767283\n",
      "347 0.19106608994273327\n",
      "348 0.19087369262536105\n",
      "349 0.19068479846062916\n",
      "350 0.19048683615775258\n",
      "351 0.19029411300482285\n",
      "352 0.1901005456145975\n",
      "353 0.18990647276656056\n",
      "354 0.18971252414385514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355 0.1895183329727786\n",
      "356 0.18932277248442636\n",
      "357 0.1891239648128644\n",
      "358 0.18890705646461\n",
      "359 0.18869572204945897\n",
      "360 0.1884817137626405\n",
      "361 0.18825945111999248\n",
      "362 0.18803898828545937\n",
      "363 0.1878180477331018\n",
      "364 0.18759639299113318\n",
      "365 0.18737389100565593\n",
      "366 0.18715061743589673\n",
      "367 0.18692257502887571\n",
      "368 0.18667982650662138\n",
      "369 0.1864419886113408\n",
      "370 0.18620252313927022\n",
      "371 0.18596063459579604\n",
      "372 0.18571522595053586\n",
      "373 0.1854631545454047\n",
      "374 0.18521378188009266\n",
      "375 0.18496209171057476\n",
      "376 0.18469232580371026\n",
      "377 0.18442817421071153\n",
      "378 0.18416588440951343\n",
      "379 0.18390497350955842\n",
      "380 0.18364538468716846\n",
      "381 0.18338721231894012\n",
      "382 0.1831281241410145\n",
      "383 0.18286776658359682\n",
      "384 0.18260457553256795\n",
      "385 0.1823319664266481\n",
      "386 0.1820439525760095\n",
      "387 0.18175966261025897\n",
      "388 0.1814761731427011\n",
      "389 0.18119618630688966\n",
      "390 0.18091866690885877\n",
      "391 0.18064198274696228\n",
      "392 0.18036531010903736\n",
      "393 0.18008949535838065\n",
      "394 0.17981528089348575\n",
      "395 0.17954134090304905\n",
      "396 0.17926187643906044\n",
      "397 0.1789688074729053\n",
      "398 0.17867107939632637\n",
      "399 0.17837966291725593\n",
      "400 0.17809428008006115\n",
      "401 0.17781252410042742\n",
      "402 0.17753235578029333\n",
      "403 0.1772544771819722\n",
      "404 0.1769804032510458\n",
      "405 0.1767097925257592\n",
      "406 0.17642939414170655\n",
      "407 0.1761539383426683\n",
      "408 0.17587636260376743\n",
      "409 0.17559527927983776\n",
      "410 0.17532127790443736\n",
      "411 0.17505345548790327\n",
      "412 0.17479106872922306\n",
      "413 0.1745293346843744\n",
      "414 0.17427456248652842\n",
      "415 0.17402418888465038\n",
      "416 0.17377628665946038\n",
      "417 0.17352472186546983\n",
      "418 0.17328070849757166\n",
      "419 0.17303845193339\n",
      "420 0.17280280789709998\n",
      "421 0.17257305669672907\n",
      "422 0.17234421510634043\n",
      "423 0.17211128599153958\n",
      "424 0.17188840930195556\n",
      "425 0.17166672590904566\n",
      "426 0.1714465351421139\n",
      "427 0.17123376516309816\n",
      "428 0.1710217027171148\n",
      "429 0.17079819509237162\n",
      "430 0.17058487751844614\n",
      "431 0.17037328031378904\n",
      "432 0.17016411198392073\n",
      "433 0.169961315563846\n",
      "434 0.16976451394152614\n",
      "435 0.16956910554568824\n",
      "436 0.1693783641814696\n",
      "437 0.16919418657432145\n",
      "438 0.16901444418029757\n",
      "439 0.1688433643014879\n",
      "440 0.16867686981551397\n",
      "441 0.1685083213534944\n",
      "442 0.16834320228341784\n",
      "443 0.1681815165434064\n",
      "444 0.1680242279438812\n",
      "445 0.1678714793127062\n",
      "446 0.1677228007926349\n",
      "447 0.1675722127721291\n",
      "448 0.16742225344970857\n",
      "449 0.1672517691938218\n",
      "450 0.16707579509807355\n",
      "451 0.1668841941196792\n",
      "452 0.16665181271043322\n",
      "453 0.16640709529375766\n",
      "454 0.1661825516190825\n",
      "455 0.16595763359236354\n",
      "456 0.1657400879903349\n",
      "457 0.16553413261992045\n",
      "458 0.16534274425714568\n",
      "459 0.16516951004767572\n",
      "460 0.16498960904829954\n",
      "461 0.16482268716581014\n",
      "462 0.16465429376738802\n",
      "463 0.1644670632182138\n",
      "464 0.16429384471615635\n",
      "465 0.16412714855061086\n",
      "466 0.16396445101080986\n",
      "467 0.16379801967256216\n",
      "468 0.16360095106729725\n",
      "469 0.16341651573185464\n",
      "470 0.16324256795650982\n",
      "471 0.1630731455706533\n",
      "472 0.16290956960109998\n",
      "473 0.16275538981567947\n",
      "474 0.16260747871350645\n",
      "475 0.16246508640034626\n",
      "476 0.16232115415028014\n",
      "477 0.1621663290919931\n",
      "478 0.16201010475407765\n",
      "479 0.16186492014271808\n",
      "480 0.16172155451679202\n",
      "481 0.16157990933656263\n",
      "482 0.16144321839220774\n",
      "483 0.16131140085295495\n",
      "484 0.16117901018455152\n",
      "485 0.16104944609005223\n",
      "486 0.1609292132654076\n",
      "487 0.16075814193900922\n",
      "488 0.16054514345800203\n",
      "489 0.16034541452794665\n",
      "490 0.16014778935043686\n",
      "491 0.1599615500353871\n",
      "492 0.1597719958305218\n",
      "493 0.1596030488921332\n",
      "494 0.1594420875630495\n",
      "495 0.15928596897934105\n",
      "496 0.15913472955419705\n",
      "497 0.1589872043210559\n",
      "498 0.15884442188225806\n",
      "499 0.15870580976161483\n",
      "500 0.15857248471328794\n",
      "501 0.15844255787465872\n",
      "502 0.1583140142594353\n",
      "503 0.1581832622508946\n",
      "504 0.15805577121144035\n",
      "505 0.15793127392515938\n",
      "506 0.1578085106750353\n",
      "507 0.15768104154360899\n",
      "508 0.15754816819096823\n",
      "509 0.15742255407253528\n",
      "510 0.1573012933106384\n",
      "511 0.1572006312423106\n",
      "512 0.15709667783211628\n",
      "513 0.15699491155954873\n",
      "514 0.15689693854257925\n",
      "515 0.15680039209340937\n",
      "516 0.15670559715779322\n",
      "517 0.15661165182155828\n",
      "518 0.15651906109977448\n",
      "519 0.15642771841690406\n",
      "520 0.1563372699746937\n",
      "521 0.1562477781848259\n",
      "522 0.15615914555481136\n",
      "523 0.15607177587869395\n",
      "524 0.1559855073240328\n",
      "525 0.15590276771347836\n",
      "526 0.1558451818422784\n",
      "527 0.15574599174106893\n",
      "528 0.1556611618912734\n",
      "529 0.15558011009802655\n",
      "530 0.1554997197677189\n",
      "531 0.15541461807793802\n",
      "532 0.1553157401845692\n",
      "533 0.15521896473961766\n",
      "534 0.15512356813538894\n",
      "535 0.15502874427110444\n",
      "536 0.15493456337597952\n",
      "537 0.15482664389361472\n",
      "538 0.15472594686207225\n",
      "539 0.15462870714853352\n",
      "540 0.15453559739432288\n",
      "541 0.15444524272131813\n",
      "542 0.15435656357025626\n",
      "543 0.15426736300272678\n",
      "544 0.1541713701046145\n",
      "545 0.15408017313923325\n",
      "546 0.1539908096710771\n",
      "547 0.15390058775836662\n",
      "548 0.15381247148818974\n",
      "549 0.15372589961543906\n",
      "550 0.1536399398055476\n",
      "551 0.15355475094185617\n",
      "552 0.1534707990670328\n",
      "553 0.1533860925533634\n",
      "554 0.15328960759060753\n",
      "555 0.15318993932593133\n",
      "556 0.15308621126159458\n",
      "557 0.15297009678020768\n",
      "558 0.15286202313872546\n",
      "559 0.1527634257712677\n",
      "560 0.15266913413010277\n",
      "561 0.15257696760240871\n",
      "562 0.15248226738105006\n",
      "563 0.15237786962717295\n",
      "564 0.15227580006007752\n",
      "565 0.15217359597784635\n",
      "566 0.15207371022143618\n",
      "567 0.15197399589352328\n",
      "568 0.1518725301309213\n",
      "569 0.15177230524373256\n",
      "570 0.15166706941569316\n",
      "571 0.15153651936055823\n",
      "572 0.15141335547405757\n",
      "573 0.1512954831074322\n",
      "574 0.15118204739080443\n",
      "575 0.15106998486197212\n",
      "576 0.15096185693748707\n",
      "577 0.15085349770800155\n",
      "578 0.15074627072464397\n",
      "579 0.15064223035897334\n",
      "580 0.15054121300365156\n",
      "581 0.15044150840874174\n",
      "582 0.150343440302725\n",
      "583 0.15024595743363023\n",
      "584 0.15015148036316667\n",
      "585 0.1500613515844411\n",
      "586 0.149956449364888\n",
      "587 0.14983653492131546\n",
      "588 0.14970416681854182\n",
      "589 0.14958054584893785\n",
      "590 0.1494646939944008\n",
      "591 0.14935236885924608\n",
      "592 0.14923885512656085\n",
      "593 0.14912789589673128\n",
      "594 0.14902038093339204\n",
      "595 0.14891476798225317\n",
      "596 0.14881118742264618\n",
      "597 0.14871039956626284\n",
      "598 0.14861056121514196\n",
      "599 0.14851093870424653\n",
      "600 0.14841370801814324\n",
      "601 0.14831914544434432\n",
      "602 0.14822569809490804\n",
      "603 0.14813101227882222\n",
      "604 0.14802612761324901\n",
      "605 0.14792389475684634\n",
      "606 0.14782190671846943\n",
      "607 0.1477194368810254\n",
      "608 0.1476231962332436\n",
      "609 0.14752852805492264\n",
      "610 0.14743505485582237\n",
      "611 0.1473428233723971\n",
      "612 0.14725094022275936\n",
      "613 0.147158192732061\n",
      "614 0.14706557363913303\n",
      "615 0.14697452799611735\n",
      "616 0.14688460869209663\n",
      "617 0.14679548741802925\n",
      "618 0.14670681491261972\n",
      "619 0.1466190509608144\n",
      "620 0.1465330237718175\n",
      "621 0.14644814637161757\n",
      "622 0.1463637317745871\n",
      "623 0.14628002263029563\n",
      "624 0.1461966558108526\n",
      "625 0.14611066653457858\n",
      "626 0.1460269315008792\n",
      "627 0.1459441933279203\n",
      "628 0.14586175955506897\n",
      "629 0.1457803689362024\n",
      "630 0.14569954794996276\n",
      "631 0.14561848185180326\n",
      "632 0.14553739449326597\n",
      "633 0.1454572064898649\n",
      "634 0.14537817785810075\n",
      "635 0.14530084135872176\n",
      "636 0.14522405742224923\n",
      "637 0.14514736934825287\n",
      "638 0.14506520754647964\n",
      "639 0.1449822076295795\n",
      "640 0.14489612872203905\n",
      "641 0.1448118709696553\n",
      "642 0.1447255441502899\n",
      "643 0.14463985343632338\n",
      "644 0.14455766478791893\n",
      "645 0.14447580692031764\n",
      "646 0.14439354493022222\n",
      "647 0.14431175481995298\n",
      "648 0.14423078960156618\n",
      "649 0.14415022109119008\n",
      "650 0.1440701730521771\n",
      "651 0.1439903669135853\n",
      "652 0.14391138760967187\n",
      "653 0.1438328063942535\n",
      "654 0.14375428065441223\n",
      "655 0.14367649930795898\n",
      "656 0.14359954155125704\n",
      "657 0.1435238988420137\n",
      "658 0.14344885899865217\n",
      "659 0.14337439116782363\n",
      "660 0.1433007298712545\n",
      "661 0.14322793544529513\n",
      "662 0.1431556565619819\n",
      "663 0.14308384310835837\n",
      "664 0.1430124067144654\n",
      "665 0.14294144489689806\n",
      "666 0.14287072323856803\n",
      "667 0.14280054147575327\n",
      "668 0.14273098373109674\n",
      "669 0.14266155840915576\n",
      "670 0.1425925053408458\n",
      "671 0.1425237612999095\n",
      "672 0.1424553556187357\n",
      "673 0.14238729186585938\n",
      "674 0.14231950593391282\n",
      "675 0.14225199013917716\n",
      "676 0.14218477438878546\n",
      "677 0.14211762497491104\n",
      "678 0.14205081318640325\n",
      "679 0.14198403060228598\n",
      "680 0.14191751929842336\n",
      "681 0.14186198399433075\n",
      "682 0.14180665262657263\n",
      "683 0.14175151853492832\n",
      "684 0.14169652676189745\n",
      "685 0.14164175964187237\n",
      "686 0.14158717537583942\n",
      "687 0.14153246990698962\n",
      "688 0.14147715309933606\n",
      "689 0.14142207853164965\n",
      "690 0.14136700216234968\n",
      "691 0.14131184686090725\n",
      "692 0.1412569065984267\n",
      "693 0.14120224553457764\n",
      "694 0.14114780908256658\n",
      "695 0.14109326179938433\n",
      "696 0.14103805832486457\n",
      "697 0.14098206048873715\n",
      "698 0.14092702850816455\n",
      "699 0.140872130537412\n",
      "700 0.14081740261426698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701 0.14076283585374053\n",
      "702 0.14070845357320433\n",
      "703 0.14065416292766345\n",
      "704 0.14059991886335435\n",
      "705 0.1405457571912446\n",
      "706 0.1404917667949744\n",
      "707 0.14043813593466903\n",
      "708 0.14038472753981715\n",
      "709 0.14033138687802454\n",
      "710 0.14027801461877626\n",
      "711 0.14022466997215668\n",
      "712 0.1401713555989042\n",
      "713 0.14011797624292732\n",
      "714 0.14006467793536231\n",
      "715 0.14001140843054688\n",
      "716 0.13995808369366547\n",
      "717 0.13990481086785056\n",
      "718 0.13985165947932182\n",
      "719 0.13979857259447445\n",
      "720 0.13974560955923582\n",
      "721 0.13969275593755032\n",
      "722 0.13964003534842936\n",
      "723 0.13958737314164624\n",
      "724 0.13953476435132775\n",
      "725 0.13948233847305339\n",
      "726 0.1394299938468571\n",
      "727 0.13937777021793835\n",
      "728 0.13932559034420933\n",
      "729 0.1392733562000332\n",
      "730 0.13922099094817855\n",
      "731 0.13916849173486026\n",
      "732 0.13911606504643317\n",
      "733 0.13906371329146752\n",
      "734 0.13901200227635419\n",
      "735 0.13896040199451723\n",
      "736 0.1389089227859938\n",
      "737 0.1388575644135198\n",
      "738 0.1388063202951665\n",
      "739 0.1387552767467167\n",
      "740 0.13870432345276953\n",
      "741 0.13865347263659433\n",
      "742 0.13860266212387504\n",
      "743 0.1385519610670504\n",
      "744 0.13850120332366433\n",
      "745 0.13845053050015438\n",
      "746 0.1383999127043717\n",
      "747 0.13834931966015185\n",
      "748 0.13829880324674923\n",
      "749 0.1382483995584886\n",
      "750 0.13819813074411097\n",
      "751 0.1381480077764322\n",
      "752 0.1380979433813116\n",
      "753 0.13804792398671462\n",
      "754 0.1379979467838913\n",
      "755 0.13794797346103238\n",
      "756 0.13789804562178326\n",
      "757 0.1378481817459667\n",
      "758 0.13779832065366718\n",
      "759 0.13774850054890017\n",
      "760 0.13769876265314693\n",
      "761 0.1376490635122243\n",
      "762 0.1375993913393975\n",
      "763 0.13754975748645748\n",
      "764 0.1375001648138702\n",
      "765 0.13745063747614789\n",
      "766 0.13740111139098618\n",
      "767 0.13735167075684412\n",
      "768 0.1373022836593942\n",
      "769 0.13725298442940467\n",
      "770 0.13720373795113586\n",
      "771 0.1371545220200226\n",
      "772 0.13710537126447503\n",
      "773 0.13705632184847322\n",
      "774 0.1370073638222417\n",
      "775 0.13695843502783148\n",
      "776 0.1369095267139784\n",
      "777 0.13686065199995895\n",
      "778 0.13681184305577762\n",
      "779 0.13676309546550713\n",
      "780 0.1367143969453493\n",
      "781 0.13666578946313226\n",
      "782 0.1366173404883932\n",
      "783 0.13656899418877755\n",
      "784 0.1365207260168609\n",
      "785 0.13647253287406838\n",
      "786 0.13642440084206478\n",
      "787 0.13637633675134322\n",
      "788 0.1363283095013111\n",
      "789 0.13628036617304412\n",
      "790 0.13623250287714186\n",
      "791 0.13618470019892032\n",
      "792 0.13613682844992955\n",
      "793 0.1360890678156288\n",
      "794 0.13604132894650295\n",
      "795 0.13599361179984898\n",
      "796 0.13594592351616644\n",
      "797 0.13589826774834002\n",
      "798 0.13585064465918026\n",
      "799 0.1358030742347781\n",
      "800 0.13575553018693576\n",
      "801 0.135708051505712\n",
      "802 0.13566059225883098\n",
      "803 0.13561316872170912\n",
      "804 0.13556584799082716\n",
      "805 0.13551859241350778\n",
      "806 0.13547135285143305\n",
      "807 0.13542416394675466\n",
      "808 0.13537700899417562\n",
      "809 0.13532971660508455\n",
      "810 0.13528234297278877\n",
      "811 0.13523507763189738\n",
      "812 0.13518782850390731\n",
      "813 0.13514064745267312\n",
      "814 0.1350935689195213\n",
      "815 0.13504645151980804\n",
      "816 0.1349994899227769\n",
      "817 0.13495257978155836\n",
      "818 0.13490551412837135\n",
      "819 0.1348578254452549\n",
      "820 0.13481024821440085\n",
      "821 0.13476273492563\n",
      "822 0.13471538411931647\n",
      "823 0.13466802595337596\n",
      "824 0.1346210315177969\n",
      "825 0.13457406337122074\n",
      "826 0.13452701985736723\n",
      "827 0.13448020915958742\n",
      "828 0.13443350031703316\n",
      "829 0.13438683650074842\n",
      "830 0.13434015266932994\n",
      "831 0.13429353653768888\n",
      "832 0.13424696702798475\n",
      "833 0.13420045349073165\n",
      "834 0.1341536022156417\n",
      "835 0.13410670237893943\n",
      "836 0.13405986015132215\n",
      "837 0.13401306534767718\n",
      "838 0.1339665158957203\n",
      "839 0.1339198199009885\n",
      "840 0.13387316537865265\n",
      "841 0.13382645682772643\n",
      "842 0.13377945115232073\n",
      "843 0.13373234961838923\n",
      "844 0.13368496707726382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m WDL(y_train, q_vec, mu, sd, pi)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(k, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "# step 2. train the model\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.SGD([alpha_matrix, mu_matrix, sd_matrix], lr=0.6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=170, gamma=0.84)\n",
    "\n",
    "# train the model\n",
    "start = time.time()\n",
    "for k in range(1000):\n",
    "    \"\"\"alpha = torch.matmul(X_train[:, 2:3], alpha_matrix)\n",
    "    mu = torch.matmul(X_train[:, [0, 4, 6]], mu_matrix)\n",
    "    sd = torch.abs(torch.matmul(torch.abs(X_train[:, [0, 1, 6]]), sd_matrix))\"\"\"\n",
    "\n",
    "    alpha = torch.matmul(X_train, alpha_matrix)\n",
    "    mu = torch.matmul(X_train, mu_matrix)\n",
    "    sd = torch.abs(torch.matmul(torch.abs(X_train), sd_matrix))\n",
    "\n",
    "    pi = torch.softmax(alpha, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = WDL(y_train, q_vec, mu, sd, pi)\n",
    "    print(k, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "print(mu)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2baf456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240, 9])\n"
     ]
    }
   ],
   "source": [
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6073d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.66676478e-01 -2.84067649e-01]\n",
      " [ 2.96840067e-01 -1.44553624e+00]\n",
      " [ 1.38995844e+00  5.06455550e-01]\n",
      " [ 1.12654493e+00  4.77331162e-01]\n",
      " [ 9.64291138e-01  1.58315217e-01]\n",
      " [ 1.81584360e-01  9.39345539e-01]\n",
      " [ 1.25845916e+00  2.35090734e-01]\n",
      " [ 5.91216707e-01  7.41017517e-01]\n",
      " [ 3.45591939e-01  1.25174015e+00]\n",
      " [ 2.51078522e-01 -5.05201003e-01]\n",
      " [-1.21620893e-01  8.20351780e-01]\n",
      " [ 7.35562548e-01  8.44545562e-01]\n",
      " [ 4.86298687e-01  1.71218464e+00]\n",
      " [ 4.67980513e-01  1.39678620e+00]\n",
      " [ 5.06183345e-01  8.41167563e-01]\n",
      " [ 2.36277541e-01  7.46228737e-01]\n",
      " [ 4.38339687e-01  1.17203483e-01]\n",
      " [ 6.32575823e-01  1.34099146e+00]\n",
      " [ 2.30677905e-01  1.51226974e-01]\n",
      " [ 9.95972070e-01  1.62690942e-01]\n",
      " [ 7.01800142e-01 -8.03170859e-02]\n",
      " [ 6.60889340e-01 -5.47164647e-01]\n",
      " [ 1.14072911e+00  8.68564505e-01]\n",
      " [ 1.01782373e+00  7.42340494e-01]\n",
      " [ 4.54636171e-01  8.62823627e-01]\n",
      " [ 4.08696982e-01  6.08927207e-01]\n",
      " [ 1.08478362e+00  5.49710235e-01]\n",
      " [ 1.07833246e+00  2.42432466e-01]\n",
      " [ 6.86554632e-01  6.36729345e-01]\n",
      " [ 8.31001655e-01 -6.16224043e-01]\n",
      " [ 5.88617060e-01  8.99398425e-02]\n",
      " [ 1.50102568e+00  8.14898376e-01]\n",
      " [ 3.87048183e-01 -3.55325288e-01]\n",
      " [ 1.13123140e+00  8.99971807e-01]\n",
      " [ 6.87263283e-01  2.18028769e-02]\n",
      " [ 9.04245361e-01  4.31148033e-01]\n",
      " [ 1.10140109e+00  5.67479771e-01]\n",
      " [ 1.01522451e+00  1.04639028e+00]\n",
      " [ 6.65714164e-01  3.00160464e-01]\n",
      " [ 1.01585280e+00  1.98427135e-01]\n",
      " [ 5.31670501e-01 -3.09686360e-01]\n",
      " [ 8.67015803e-01  9.55342337e-01]\n",
      " [ 8.11512036e-01 -2.00464207e-01]\n",
      " [ 8.65373617e-01  1.84393484e-01]\n",
      " [ 8.43953797e-01 -5.28067403e-01]\n",
      " [ 5.52101821e-01  1.25271640e-01]\n",
      " [ 2.89013210e-01 -1.07190006e+00]\n",
      " [ 1.09435265e+00 -3.47412938e-01]\n",
      " [ 6.29817327e-01 -7.50613173e-01]\n",
      " [ 7.37000443e-01  5.46739517e-01]\n",
      " [ 1.17558789e+00 -2.93839427e-01]\n",
      " [ 1.16549882e+00 -3.59361066e-02]\n",
      " [ 8.95669615e-01 -3.63883718e-01]\n",
      " [ 7.96241922e-02 -1.31680865e+00]\n",
      " [ 1.23603870e-01  1.96191325e-01]\n",
      " [ 1.22074526e+00  1.56017644e+00]\n",
      " [ 1.95661720e-01 -1.00570909e+00]\n",
      " [ 7.25354014e-01  3.57140082e-01]\n",
      " [ 1.75805851e-01 -4.22173371e-01]\n",
      " [ 1.49191869e+00  1.18305408e+00]\n",
      " [ 7.48938247e-01  6.42749103e-01]\n",
      " [ 2.94926917e-01 -2.64227470e-01]\n",
      " [ 1.13953699e+00  3.01193572e-01]\n",
      " [ 1.08055152e+00 -4.29461638e-01]\n",
      " [ 5.06812633e-01  2.25363427e-01]\n",
      " [ 7.03806123e-01  8.50291291e-01]\n",
      " [ 1.88491446e-01  1.81061789e-02]\n",
      " [ 7.40992629e-01  1.64281756e+00]\n",
      " [ 1.16856783e+00  4.26076467e-02]\n",
      " [ 9.85874644e-01 -5.21718817e-01]\n",
      " [ 5.91108790e-01 -8.44907551e-01]\n",
      " [ 1.75734326e-01 -6.27463220e-01]\n",
      " [ 1.06244483e-01 -6.83473832e-01]\n",
      " [ 1.04618286e+00 -2.26481386e-01]\n",
      " [ 3.59630791e-01 -1.04965029e-01]\n",
      " [ 1.55938289e+00  1.11636906e+00]\n",
      " [ 1.04626908e+00  1.06453088e+00]\n",
      " [ 9.24362425e-01  8.90742065e-01]\n",
      " [ 6.75130799e-01 -1.03779868e+00]\n",
      " [ 1.85299569e-02 -4.33882016e-01]\n",
      " [ 6.84710744e-01  1.45834775e+00]\n",
      " [ 2.54253637e-01 -1.29180601e+00]\n",
      " [ 4.07006495e-01  6.23413119e-03]\n",
      " [ 7.82031571e-01  1.03727767e-01]\n",
      " [ 5.61378113e-01  3.54509100e-01]\n",
      " [ 7.63771599e-01 -4.75967957e-01]\n",
      " [ 1.25894842e-01 -6.39451523e-01]\n",
      " [ 9.60234698e-01  7.39446804e-01]\n",
      " [ 1.29654195e-01 -2.27918569e-01]\n",
      " [ 4.30775187e-01  6.41942820e-02]\n",
      " [ 1.25247739e+00  7.66114575e-01]\n",
      " [ 7.43694307e-01  1.28410146e+00]\n",
      " [ 3.35126681e-01  3.61030447e-01]\n",
      " [ 1.10275749e+00  5.97852405e-01]\n",
      " [ 8.84030559e-01 -3.15058034e-01]\n",
      " [ 4.99227457e-01 -8.19588427e-01]\n",
      " [ 6.21525167e-01  7.49892960e-01]\n",
      " [ 1.54928190e+00  1.53184161e-01]\n",
      " [ 9.72581581e-01  1.33167750e-02]\n",
      " [ 1.15582349e+00  3.47621182e-01]\n",
      " [ 1.14523285e+00  9.62398211e-01]\n",
      " [ 7.48075202e-01 -2.37518104e-01]\n",
      " [ 1.28694745e+00  4.06295915e-01]\n",
      " [ 8.65770184e-01  7.79061944e-01]\n",
      " [ 3.82459414e-01  1.23510854e+00]\n",
      " [ 5.12525566e-01  2.89482338e-01]\n",
      " [ 4.92137645e-01 -1.90297690e-01]\n",
      " [ 5.99636808e-01  6.57056689e-01]\n",
      " [ 5.65666813e-01 -9.21166761e-02]\n",
      " [ 5.14925486e-01 -6.45005554e-01]\n",
      " [ 1.38700937e+00  7.24352583e-01]\n",
      " [-2.91556640e-01  1.22267498e-03]\n",
      " [ 1.00789098e+00  4.71142899e-02]\n",
      " [ 1.27631398e+00  1.55555105e+00]\n",
      " [ 3.01702146e-01 -1.54098449e-01]\n",
      " [ 3.17456944e-01  3.97084103e-01]\n",
      " [-1.51975399e-02 -3.43171321e-01]\n",
      " [ 7.01642388e-01 -1.03296762e-01]\n",
      " [ 7.17833413e-01  3.36172815e-01]\n",
      " [ 7.05864116e-01 -6.18027358e-01]\n",
      " [ 8.31185646e-01  1.53345455e-01]\n",
      " [ 7.40407683e-01  7.89430620e-02]\n",
      " [ 8.49052682e-01  2.32465129e-01]\n",
      " [ 8.75653589e-01 -2.02181925e-01]\n",
      " [ 5.30132514e-01 -9.85146928e-01]\n",
      " [ 8.85088748e-01  5.85234776e-01]\n",
      " [ 3.47610589e-01  4.50786416e-01]\n",
      " [ 5.58939258e-01 -2.74148197e-01]\n",
      " [ 5.36718093e-01  6.16678292e-01]\n",
      " [ 5.77342529e-01  5.53546644e-01]\n",
      " [ 5.66515840e-01  1.09453347e+00]\n",
      " [ 1.24234962e+00  8.34868423e-01]\n",
      " [ 2.09940518e-01  6.00888479e-01]\n",
      " [ 7.92949778e-01  1.13396364e+00]\n",
      " [ 6.60608827e-01  4.18421525e-01]\n",
      " [ 2.07241985e-01 -1.66432507e-01]\n",
      " [ 1.44986611e+00  5.72117508e-01]\n",
      " [ 7.45340023e-01  6.29291813e-01]\n",
      " [ 1.10161484e+00  2.13710845e-01]\n",
      " [ 6.46301615e-01 -2.32430173e-01]\n",
      " [ 6.51297711e-01  1.37848394e-01]\n",
      " [ 6.40929321e-01 -1.20804982e-01]\n",
      " [ 1.08423609e+00  2.49683861e-01]\n",
      " [ 5.82227223e-01  1.22327496e+00]\n",
      " [ 4.98347968e-01  7.60460868e-01]\n",
      " [ 1.15804710e+00  2.09228087e-01]\n",
      " [ 2.88085055e-01 -3.28280300e-01]\n",
      " [ 1.42332638e+00  1.63251559e+00]\n",
      " [ 7.52051555e-01  5.92421348e-01]\n",
      " [ 7.20121917e-01  1.61220341e+00]\n",
      " [ 7.99449815e-01  8.17175215e-01]\n",
      " [ 9.19828740e-01  6.60250314e-01]\n",
      " [ 5.99812705e-01 -6.11859543e-01]\n",
      " [ 8.82314093e-01  8.97241013e-01]\n",
      " [ 1.24238044e+00  3.57607658e-02]\n",
      " [-6.95916981e-03  1.99153598e-01]\n",
      " [ 1.09817454e+00  3.64282560e-01]\n",
      " [ 1.23657197e+00  3.06494637e-01]\n",
      " [ 5.57262463e-01  1.52499418e+00]\n",
      " [ 8.23646738e-01  1.02045525e+00]\n",
      " [ 1.31990558e-01  7.55791700e-02]\n",
      " [ 7.44299676e-01 -1.29689196e-01]\n",
      " [ 5.13592588e-01  1.14173354e+00]\n",
      " [ 1.80944680e-02 -4.77484029e-01]\n",
      " [ 7.10823894e-02 -2.23307965e-01]\n",
      " [ 3.45978566e-01 -7.29805613e-01]\n",
      " [ 7.98330881e-01  1.71631671e-01]\n",
      " [ 7.94071063e-01  7.31566472e-01]\n",
      " [ 6.63134921e-01  1.94366169e-01]\n",
      " [ 3.17653757e-01 -1.65667211e-01]\n",
      " [ 6.68188656e-01 -6.32084754e-01]\n",
      " [ 9.85000640e-01  1.00923036e-01]\n",
      " [ 3.34016149e-01  1.17589228e+00]\n",
      " [ 2.54091858e-01  2.99985261e-02]\n",
      " [ 4.58324580e-01 -1.10817072e+00]\n",
      " [ 1.02012950e+00  8.58916459e-01]\n",
      " [ 6.11897241e-01  5.61174141e-02]\n",
      " [ 5.41188887e-01  6.19114087e-01]\n",
      " [ 9.63490376e-01  1.48607041e-01]\n",
      " [ 1.01993478e+00  7.72895490e-01]\n",
      " [ 1.29400794e+00  1.31681406e+00]\n",
      " [ 6.64071923e-01 -1.27399390e-01]\n",
      " [ 2.48569669e-01  1.16469739e-01]\n",
      " [ 7.24721559e-01  3.51877869e-01]\n",
      " [ 1.03627876e+00  6.76607273e-02]\n",
      " [ 5.22978607e-01  2.22526362e-01]\n",
      " [ 1.58871027e-01  1.05429315e+00]\n",
      " [ 8.35094693e-01 -2.90359001e-01]\n",
      " [ 1.07409402e+00 -2.00498107e-02]\n",
      " [ 4.95702307e-01  4.60251995e-01]\n",
      " [ 9.52794871e-01  3.36471947e-01]\n",
      " [ 3.08897096e-01 -3.20022272e-02]\n",
      " [ 1.24162737e+00  1.47422675e+00]\n",
      " [ 2.68154031e-01  1.87097161e-01]\n",
      " [ 6.58581935e-01  1.53561498e+00]\n",
      " [-6.79319005e-02  1.78451105e-01]\n",
      " [ 1.18532207e+00 -1.34637966e-01]\n",
      " [ 9.95882893e-01  5.50174194e-01]\n",
      " [ 1.37454171e+00  1.06077528e+00]\n",
      " [ 1.01045271e+00  1.40390501e-01]\n",
      " [ 2.41837929e-01 -7.55437672e-01]\n",
      " [ 5.62039501e-01 -8.09613186e-02]\n",
      " [ 9.12755961e-01 -5.14000783e-01]\n",
      " [ 9.85800665e-01  8.47859688e-01]\n",
      " [ 6.82249724e-01 -8.21838139e-02]\n",
      " [ 3.59048923e-01  1.39609942e+00]\n",
      " [ 6.50717304e-01 -6.44007456e-01]\n",
      " [ 1.29203224e+00  1.21823371e+00]\n",
      " [ 1.28462079e+00  6.85824115e-01]\n",
      " [ 4.63706973e-01  1.61536951e+00]\n",
      " [ 1.04238298e+00  2.52454985e-01]\n",
      " [ 6.53019376e-01  5.95785805e-01]\n",
      " [ 4.59195457e-01  1.26417521e+00]\n",
      " [ 1.17509090e+00 -4.19061386e-01]\n",
      " [ 8.57042395e-01 -2.85103080e-01]\n",
      " [-2.47269825e-01  6.04402158e-01]\n",
      " [ 8.36254927e-01  4.43099211e-01]\n",
      " [ 1.25809887e+00  9.09069473e-01]\n",
      " [ 7.31379238e-01  1.81333254e+00]\n",
      " [ 1.29079190e+00  1.15323554e+00]\n",
      " [ 1.00073786e+00  1.91726358e-02]\n",
      " [ 1.05743365e+00  1.21282788e+00]\n",
      " [ 1.21164665e+00  1.05418760e+00]\n",
      " [ 1.50318171e+00  1.39506095e+00]\n",
      " [ 5.43517838e-01  5.10524798e-01]\n",
      " [ 1.30131420e+00  1.42141064e+00]\n",
      " [ 9.94610713e-01  2.19764596e-01]\n",
      " [ 5.16476813e-01 -9.02186235e-01]\n",
      " [ 2.09295980e-01 -4.80751507e-01]\n",
      " [ 1.69175457e-01 -2.03171049e-01]\n",
      " [ 1.36661816e+00  4.31372372e-01]\n",
      " [ 9.46321427e-01  4.43904586e-01]\n",
      " [ 7.59925918e-01  6.67377916e-01]\n",
      " [ 2.28636170e-01  3.10026053e-02]\n",
      " [ 8.95990241e-01  2.88184026e-01]\n",
      " [ 9.14892596e-01  2.35132156e-01]\n",
      " [ 3.90205398e-01 -7.23189725e-01]\n",
      " [ 1.34508092e+00  1.03968020e+00]\n",
      " [ 1.38328364e+00  4.93955953e-01]\n",
      " [ 7.42534280e-01 -5.02123167e-01]]\n",
      "[[ 0.49559938 -0.42316504]\n",
      " [-0.11273809 -0.51299145]\n",
      " [ 0.53740975 -0.58132721]\n",
      " [ 0.6611832  -0.60083302]\n",
      " [ 0.41861169 -0.7265914 ]\n",
      " [ 0.30342228 -0.31602184]\n",
      " [ 0.22355985 -0.13575857]\n",
      " [ 0.64828661 -0.37668027]\n",
      " [ 0.29809458 -0.20990768]\n",
      " [ 0.26617435 -0.39212737]\n",
      " [-0.0106511  -0.11740091]\n",
      " [ 0.4848837  -0.63733094]\n",
      " [ 0.16533745 -0.16037671]\n",
      " [ 0.40529704 -0.49730716]\n",
      " [ 0.51862553 -0.66086002]\n",
      " [ 0.35758847 -0.39726488]\n",
      " [ 0.16958399  0.17571125]\n",
      " [ 0.50038628 -0.53234472]\n",
      " [ 0.10209924  0.19542257]\n",
      " [ 0.51735235 -0.64257661]\n",
      " [ 0.4504563  -0.11133658]\n",
      " [ 0.36494302 -0.52003668]\n",
      " [ 0.63964192 -0.35646007]\n",
      " [ 0.54462625 -0.41941512]\n",
      " [ 0.49937078 -0.61971084]\n",
      " [ 0.22353143  0.09889857]\n",
      " [ 0.63309548 -0.24555241]\n",
      " [ 0.29558645 -0.5824893 ]\n",
      " [ 0.69338944 -0.60096936]\n",
      " [ 0.1632121  -0.42089355]\n",
      " [ 0.21515413  0.22136791]\n",
      " [ 0.62175942 -0.71450603]\n",
      " [ 0.36651746 -0.53761909]\n",
      " [ 0.55927784 -0.22021982]\n",
      " [ 0.42888595 -0.08504471]\n",
      " [ 0.46135947 -0.20743218]\n",
      " [ 0.6952901  -0.39525871]\n",
      " [ 0.59738716 -0.56594342]\n",
      " [ 0.15679041  0.22354354]\n",
      " [ 0.49334929 -0.17514485]\n",
      " [ 0.36541468 -0.21335203]\n",
      " [ 0.62828683 -0.28623526]\n",
      " [-0.02287179 -0.41938633]\n",
      " [ 0.44018424 -0.03163113]\n",
      " [ 0.22723759 -0.26190464]\n",
      " [ 0.27672775  0.10354546]\n",
      " [ 0.07547295 -0.46742246]\n",
      " [ 0.31751214  0.06347208]\n",
      " [ 0.12580113 -0.42109011]\n",
      " [ 0.10359268 -0.38636539]\n",
      " [ 0.34824801 -0.60450282]\n",
      " [ 0.11477538  0.00624238]\n",
      " [ 0.42653103 -0.52882191]\n",
      " [-0.19191502 -0.18583193]\n",
      " [ 0.30668293 -0.26597864]\n",
      " [ 0.25383832 -0.15811977]\n",
      " [ 0.04423507 -0.59601068]\n",
      " [ 0.19542204  0.2173057 ]\n",
      " [ 0.22899594 -0.20460463]\n",
      " [ 0.5346223  -0.42211085]\n",
      " [ 0.28188993  0.06911778]\n",
      " [ 0.1664481   0.11249797]\n",
      " [ 0.68852429 -0.5970193 ]\n",
      " [ 0.35584645 -0.59352485]\n",
      " [ 0.3056346  -0.02327684]\n",
      " [ 0.55929398 -0.14738451]\n",
      " [ 0.21930136  0.02766436]\n",
      " [ 0.41176293 -0.42712858]\n",
      " [ 0.5188413  -0.45624665]\n",
      " [ 0.3932824  -0.63284927]\n",
      " [ 0.2104313  -0.56016724]\n",
      " [ 0.1248943  -0.01972888]\n",
      " [ 0.00772054  0.06012507]\n",
      " [ 0.4396402  -0.57836489]\n",
      " [-0.01960955  0.34937622]\n",
      " [ 0.59049875 -0.70611246]\n",
      " [ 0.33327933 -0.0626968 ]\n",
      " [ 0.71202349 -0.56774853]\n",
      " [ 0.09639431 -0.05362052]\n",
      " [-0.14905849  0.35692606]\n",
      " [ 0.5543358  -0.3329698 ]\n",
      " [-0.03116528 -0.56070051]\n",
      " [ 0.26034858 -0.02968439]\n",
      " [ 0.47980526 -0.11830403]\n",
      " [ 0.56451331 -0.24332724]\n",
      " [ 0.37550416 -0.58681733]\n",
      " [-0.00676186  0.12456159]\n",
      " [ 0.50432461 -0.16061447]\n",
      " [ 0.10940638  0.1556402 ]\n",
      " [ 0.2962948   0.068415  ]\n",
      " [ 0.67875763 -0.57769091]\n",
      " [ 0.40882303 -0.04902712]\n",
      " [-0.01590624  0.04940175]\n",
      " [ 0.5092389  -0.6269056 ]\n",
      " [ 0.48173038 -0.66990613]\n",
      " [ 0.20633697 -0.04017555]\n",
      " [ 0.07452077 -0.11761544]\n",
      " [ 0.30294373 -0.01683339]\n",
      " [ 0.40154858 -0.26307586]\n",
      " [ 0.45412096 -0.67871955]\n",
      " [ 0.56851719 -0.41164033]\n",
      " [ 0.15722109 -0.68921435]\n",
      " [ 0.50823849 -0.45417635]\n",
      " [ 0.45474047 -0.20838816]\n",
      " [ 0.36828338 -0.21776212]\n",
      " [ 0.59358815 -0.37080274]\n",
      " [ 0.3035391   0.00547207]\n",
      " [ 0.18446989 -0.00399766]\n",
      " [ 0.48157655 -0.3537203 ]\n",
      " [ 0.17221864 -0.56084876]\n",
      " [ 0.66762229 -0.72196473]\n",
      " [-0.19798926  0.20968848]\n",
      " [ 0.36382001 -0.56478524]\n",
      " [ 0.09418812 -0.11867374]\n",
      " [ 0.118714    0.2146226 ]\n",
      " [-0.14502552  0.01143056]\n",
      " [-0.04912467  0.22286247]\n",
      " [ 0.45950699 -0.13866452]\n",
      " [ 0.4480177  -0.01685609]\n",
      " [ 0.36114537 -0.63467299]\n",
      " [ 0.51348198 -0.2000975 ]\n",
      " [ 0.28120956 -0.55411305]\n",
      " [ 0.36919354 -0.5767187 ]\n",
      " [ 0.16455387 -0.41392134]\n",
      " [ 0.15535219 -0.38477194]\n",
      " [ 0.50170948 -0.36931356]\n",
      " [ 0.47247057 -0.41630854]\n",
      " [ 0.49267018 -0.62129362]\n",
      " [-0.13802516 -0.16937493]\n",
      " [ 0.58108258 -0.71167046]\n",
      " [ 0.52533558 -0.21531913]\n",
      " [ 0.70335953 -0.54272162]\n",
      " [ 0.35587192 -0.29326459]\n",
      " [ 0.42878298 -0.64927316]\n",
      " [ 0.43234928 -0.00346125]\n",
      " [ 0.05351788  0.1627154 ]\n",
      " [ 0.60927187 -0.5931989 ]\n",
      " [ 0.70199561 -0.64486815]\n",
      " [ 0.55515616 -0.77649789]\n",
      " [ 0.39075883 -0.16050027]\n",
      " [ 0.29526561 -0.38894183]\n",
      " [ 0.31299614 -0.48127492]\n",
      " [ 0.46109125 -0.72192205]\n",
      " [ 0.48535949 -0.66813958]\n",
      " [-0.1931408  -0.06998269]\n",
      " [ 0.22824911 -0.36539306]\n",
      " [ 0.27870983 -0.67577384]\n",
      " [ 0.39206274 -0.25191809]\n",
      " [ 0.25917209  0.1239459 ]\n",
      " [ 0.43318085 -0.42709436]\n",
      " [ 0.30054519 -0.22337457]\n",
      " [ 0.42781394 -0.41046262]\n",
      " [ 0.30429961 -0.43334599]\n",
      " [ 0.62666855 -0.74141742]\n",
      " [ 0.53970993 -0.57839261]\n",
      " [ 0.12199669 -0.00292138]\n",
      " [ 0.60216717 -0.27019011]\n",
      " [ 0.59000879 -0.40343602]\n",
      " [ 0.33151899 -0.22303921]\n",
      " [ 0.14137494 -0.1470847 ]\n",
      " [ 0.26711232 -0.21707255]\n",
      " [ 0.4942152  -0.30180421]\n",
      " [ 0.54739518 -0.36265005]\n",
      " [ 0.02254961  0.15855072]\n",
      " [-0.09033077  0.22284887]\n",
      " [ 0.15214264 -0.12263812]\n",
      " [ 0.43669585 -0.03725678]\n",
      " [ 0.53198714 -0.10335555]\n",
      " [ 0.37976344  0.03028866]\n",
      " [ 0.21263108  0.0693179 ]\n",
      " [ 0.34116961 -0.45498975]\n",
      " [ 0.46045626 -0.76427701]\n",
      " [ 0.24367059 -0.13112219]\n",
      " [ 0.0456286   0.17566192]\n",
      " [ 0.06451383 -0.45424609]\n",
      " [ 0.47450767 -0.06296749]\n",
      " [ 0.26833689 -0.3912188 ]\n",
      " [ 0.09661894 -0.08016274]\n",
      " [ 0.64438186 -0.39009662]\n",
      " [ 0.56406629 -0.3748542 ]\n",
      " [ 0.49533955 -0.2264499 ]\n",
      " [ 0.44168894 -0.12437076]\n",
      " [ 0.3361418  -0.11800676]\n",
      " [ 0.45855453 -0.08355237]\n",
      " [ 0.63365754 -0.49620922]\n",
      " [ 0.01567605  0.33762128]\n",
      " [ 0.18470684 -0.43108932]\n",
      " [ 0.44685263 -0.08330259]\n",
      " [ 0.4790291  -0.35331743]\n",
      " [-0.00756504  0.33209814]\n",
      " [ 0.61661864 -0.71013491]\n",
      " [ 0.12868256  0.12541233]\n",
      " [ 0.16602918 -0.12460276]\n",
      " [ 0.13970452  0.16517342]\n",
      " [ 0.39365041 -0.15571591]\n",
      " [-0.01187318  0.14028941]\n",
      " [ 0.45556213 -0.18143672]\n",
      " [ 0.47200236 -0.0370069 ]\n",
      " [ 0.46193809 -0.62754484]\n",
      " [ 0.59904631 -0.30985916]\n",
      " [ 0.06796716 -0.12907523]\n",
      " [ 0.29034591  0.08403566]\n",
      " [ 0.09705448  0.06770071]\n",
      " [ 0.42714362 -0.05975276]\n",
      " [ 0.46552684 -0.31449853]\n",
      " [ 0.28082104 -0.32626936]\n",
      " [ 0.30094166 -0.48625891]\n",
      " [ 0.58660058 -0.45810805]\n",
      " [ 0.70768725 -0.43585117]\n",
      " [ 0.09297226 -0.14412754]\n",
      " [ 0.40653227 -0.08697192]\n",
      " [ 0.67556432 -0.48748655]\n",
      " [ 0.4215637  -0.55811078]\n",
      " [ 0.40345039  0.05037991]\n",
      " [ 0.44465062 -0.46202566]\n",
      " [-0.12918871 -0.00828576]\n",
      " [ 0.28982974 -0.49226184]\n",
      " [ 0.64087936 -0.32300693]\n",
      " [ 0.39165099 -0.28471044]\n",
      " [ 0.65447896 -0.36908522]\n",
      " [ 0.23064261 -0.42670117]\n",
      " [ 0.36488391 -0.29384459]\n",
      " [ 0.4280767  -0.11185274]\n",
      " [ 0.46158797 -0.39816656]\n",
      " [ 0.47445331 -0.67844196]\n",
      " [ 0.29550749 -0.14862769]\n",
      " [ 0.24589021 -0.71188451]\n",
      " [ 0.23110451 -0.54986686]\n",
      " [ 0.21648756 -0.40845435]\n",
      " [ 0.00235076  0.2307391 ]\n",
      " [ 0.33760685 -0.24691168]\n",
      " [ 0.4250715  -0.08902427]\n",
      " [ 0.2320735   0.07249508]\n",
      " [ 0.09938403 -0.34536016]\n",
      " [ 0.24193321  0.05933812]\n",
      " [ 0.48229945 -0.11633101]\n",
      " [ 0.17722446 -0.26405575]\n",
      " [ 0.65153328 -0.35308897]\n",
      " [ 0.66270597 -0.76777898]\n",
      " [ 0.28009251 -0.33332374]]\n",
      "[[ 0.17562138 -0.17562138]\n",
      " [ 0.11513419 -0.11513419]\n",
      " [ 0.46325811 -0.46325811]\n",
      " [ 0.30946536 -0.30946536]\n",
      " [ 0.29168514 -0.29168514]\n",
      " [ 0.14898574 -0.14898574]\n",
      " [ 0.45129206 -0.45129206]\n",
      " [ 0.19281202 -0.19281202]\n",
      " [ 0.25186436 -0.25186436]\n",
      " [ 0.07697861 -0.07697861]\n",
      " [ 0.08930626 -0.08930626]\n",
      " [ 0.21432148 -0.21432148]\n",
      " [ 0.16672085 -0.16672085]\n",
      " [ 0.19207466 -0.19207466]\n",
      " [ 0.23869326 -0.23869326]\n",
      " [ 0.21994041 -0.21994041]\n",
      " [ 0.08221411 -0.08221411]\n",
      " [ 0.1930163  -0.1930163 ]\n",
      " [ 0.0610265  -0.0610265 ]\n",
      " [ 0.2642564  -0.2642564 ]\n",
      " [ 0.13539454 -0.13539454]\n",
      " [ 0.18273957 -0.18273957]\n",
      " [ 0.43246908 -0.43246908]\n",
      " [ 0.42716707 -0.42716707]\n",
      " [ 0.19499425 -0.19499425]\n",
      " [ 0.11214414 -0.11214414]\n",
      " [ 0.35205731 -0.35205731]\n",
      " [ 0.36851777 -0.36851777]\n",
      " [ 0.2082339  -0.2082339 ]\n",
      " [ 0.22921692 -0.22921692]\n",
      " [ 0.23281693 -0.23281693]\n",
      " [ 0.36001271 -0.36001271]\n",
      " [ 0.197569   -0.197569  ]\n",
      " [ 0.22870722 -0.22870722]\n",
      " [ 0.25635058 -0.25635058]\n",
      " [ 0.22866142 -0.22866142]\n",
      " [ 0.25959187 -0.25959187]\n",
      " [ 0.23471907 -0.23471907]\n",
      " [ 0.0457702  -0.0457702 ]\n",
      " [ 0.27336073 -0.27336073]\n",
      " [ 0.11466185 -0.11466185]\n",
      " [ 0.1606031  -0.1606031 ]\n",
      " [ 0.31395353 -0.31395353]\n",
      " [ 0.11725699 -0.11725699]\n",
      " [ 0.12611632 -0.12611632]\n",
      " [ 0.08969026 -0.08969026]\n",
      " [ 0.12858971 -0.12858971]\n",
      " [ 0.39031729 -0.39031729]\n",
      " [ 0.21574177 -0.21574177]\n",
      " [ 0.20448077 -0.20448077]\n",
      " [ 0.23964713 -0.23964713]\n",
      " [ 0.44234345 -0.44234345]\n",
      " [ 0.28135988 -0.28135988]\n",
      " [ 0.03928129 -0.03928129]\n",
      " [ 0.07037807 -0.07037807]\n",
      " [ 0.50041356 -0.50041356]\n",
      " [ 0.09265345 -0.09265345]\n",
      " [ 0.22589588 -0.22589588]\n",
      " [ 0.03165233 -0.03165233]\n",
      " [ 0.28053438 -0.28053438]\n",
      " [ 0.1472515  -0.1472515 ]\n",
      " [ 0.03305135 -0.03305135]\n",
      " [ 0.30198844 -0.30198844]\n",
      " [ 0.2069553  -0.2069553 ]\n",
      " [ 0.10760839 -0.10760839]\n",
      " [ 0.29140412 -0.29140412]\n",
      " [ 0.20827469 -0.20827469]\n",
      " [ 0.22720742 -0.22720742]\n",
      " [ 0.2784397  -0.2784397 ]\n",
      " [ 0.27055124 -0.27055124]\n",
      " [ 0.22449323 -0.22449323]\n",
      " [ 0.00158135 -0.00158135]\n",
      " [-0.0615988   0.0615988 ]\n",
      " [ 0.2503366  -0.2503366 ]\n",
      " [ 0.01065639 -0.01065639]\n",
      " [ 0.47901883 -0.47901883]\n",
      " [ 0.41884213 -0.41884213]\n",
      " [ 0.25946945 -0.25946945]\n",
      " [ 0.236135   -0.236135  ]\n",
      " [-0.05213956  0.05213956]\n",
      " [ 0.26443466 -0.26443466]\n",
      " [ 0.0930538  -0.0930538 ]\n",
      " [ 0.05968336 -0.05968336]\n",
      " [ 0.24877179 -0.24877179]\n",
      " [ 0.26448398 -0.26448398]\n",
      " [ 0.21907432 -0.21907432]\n",
      " [ 0.12696558 -0.12696558]\n",
      " [ 0.20805209 -0.20805209]\n",
      " [ 0.0058021  -0.0058021 ]\n",
      " [ 0.25472967 -0.25472967]\n",
      " [ 0.44505271 -0.44505271]\n",
      " [ 0.27847919 -0.27847919]\n",
      " [ 0.29834069 -0.29834069]\n",
      " [ 0.32922981 -0.32922981]\n",
      " [ 0.31211275 -0.31211275]\n",
      " [ 0.22239854 -0.22239854]\n",
      " [ 0.18599107 -0.18599107]\n",
      " [ 0.36912262 -0.36912262]\n",
      " [ 0.27012305 -0.27012305]\n",
      " [ 0.37772161 -0.37772161]\n",
      " [ 0.46165558 -0.46165558]\n",
      " [ 0.21902605 -0.21902605]\n",
      " [ 0.30372542 -0.30372542]\n",
      " [ 0.23574636 -0.23574636]\n",
      " [ 0.26140854 -0.26140854]\n",
      " [ 0.16250256 -0.16250256]\n",
      " [ 0.10594718 -0.10594718]\n",
      " [ 0.39703467 -0.39703467]\n",
      " [ 0.24897773 -0.24897773]\n",
      " [ 0.17542365 -0.17542365]\n",
      " [ 0.45259956 -0.45259956]\n",
      " [-0.08143008  0.08143008]\n",
      " [ 0.38158839 -0.38158839]\n",
      " [ 0.47599935 -0.47599935]\n",
      " [ 0.04906237 -0.04906237]\n",
      " [ 0.05448519 -0.05448519]\n",
      " [ 0.13631886 -0.13631886]\n",
      " [ 0.139871   -0.139871  ]\n",
      " [ 0.1074852  -0.1074852 ]\n",
      " [ 0.1978452  -0.1978452 ]\n",
      " [ 0.31228004 -0.31228004]\n",
      " [ 0.19311888 -0.19311888]\n",
      " [ 0.23737445 -0.23737445]\n",
      " [ 0.29435756 -0.29435756]\n",
      " [ 0.14012896 -0.14012896]\n",
      " [ 0.22488483 -0.22488483]\n",
      " [ 0.22670118 -0.22670118]\n",
      " [ 0.18587662 -0.18587662]\n",
      " [ 0.13501898 -0.13501898]\n",
      " [ 0.24335256 -0.24335256]\n",
      " [ 0.28008162 -0.28008162]\n",
      " [ 0.44698825 -0.44698825]\n",
      " [ 0.21943643 -0.21943643]\n",
      " [ 0.21602344 -0.21602344]\n",
      " [ 0.27299225 -0.27299225]\n",
      " [ 0.20294896 -0.20294896]\n",
      " [ 0.39995441 -0.39995441]\n",
      " [ 0.22536348 -0.22536348]\n",
      " [ 0.32963647 -0.32963647]\n",
      " [ 0.14931184 -0.14931184]\n",
      " [ 0.27307451 -0.27307451]\n",
      " [ 0.14456858 -0.14456858]\n",
      " [ 0.40908246 -0.40908246]\n",
      " [ 0.20475598 -0.20475598]\n",
      " [ 0.37677114 -0.37677114]\n",
      " [ 0.44299578 -0.44299578]\n",
      " [ 0.180849   -0.180849  ]\n",
      " [ 0.47858104 -0.47858104]\n",
      " [ 0.12237016 -0.12237016]\n",
      " [ 0.23059362 -0.23059362]\n",
      " [ 0.23384466 -0.23384466]\n",
      " [ 0.25972811 -0.25972811]\n",
      " [ 0.23172355 -0.23172355]\n",
      " [ 0.25632454 -0.25632454]\n",
      " [ 0.2301863  -0.2301863 ]\n",
      " [ 0.17517143 -0.17517143]\n",
      " [ 0.31586555 -0.31586555]\n",
      " [ 0.19825527 -0.19825527]\n",
      " [ 0.24738367 -0.24738367]\n",
      " [ 0.47362846 -0.47362846]\n",
      " [ 0.1708726  -0.1708726 ]\n",
      " [ 0.26230768 -0.26230768]\n",
      " [ 0.19130982 -0.19130982]\n",
      " [-0.05541658  0.05541658]\n",
      " [ 0.15564912 -0.15564912]\n",
      " [ 0.0722859  -0.0722859 ]\n",
      " [ 0.28938153 -0.28938153]\n",
      " [ 0.26915549 -0.26915549]\n",
      " [ 0.26186819 -0.26186819]\n",
      " [ 0.22275926 -0.22275926]\n",
      " [ 0.22780142 -0.22780142]\n",
      " [ 0.37098789 -0.37098789]\n",
      " [ 0.26160996 -0.26160996]\n",
      " [ 0.25652501 -0.25652501]\n",
      " [ 0.14725099 -0.14725099]\n",
      " [ 0.18727224 -0.18727224]\n",
      " [ 0.12553105 -0.12553105]\n",
      " [ 0.3638699  -0.3638699 ]\n",
      " [ 0.22295262 -0.22295262]\n",
      " [ 0.42808406 -0.42808406]\n",
      " [ 0.26646475 -0.26646475]\n",
      " [ 0.13246275 -0.13246275]\n",
      " [ 0.21177313 -0.21177313]\n",
      " [ 0.33503034 -0.33503034]\n",
      " [ 0.33174295 -0.33174295]\n",
      " [ 0.24575012 -0.24575012]\n",
      " [ 0.20785694 -0.20785694]\n",
      " [ 0.24705672 -0.24705672]\n",
      " [ 0.26313063 -0.26313063]\n",
      " [ 0.03382767 -0.03382767]\n",
      " [ 0.27289314 -0.27289314]\n",
      " [ 0.24527568 -0.24527568]\n",
      " [ 0.47175764 -0.47175764]\n",
      " [ 0.26170125 -0.26170125]\n",
      " [ 0.26867952 -0.26867952]\n",
      " [ 0.01032721 -0.01032721]\n",
      " [ 0.39960602 -0.39960602]\n",
      " [ 0.31133887 -0.31133887]\n",
      " [ 0.39756507 -0.39756507]\n",
      " [ 0.19531886 -0.19531886]\n",
      " [ 0.16924814 -0.16924814]\n",
      " [ 0.23690323 -0.23690323]\n",
      " [ 0.34402086 -0.34402086]\n",
      " [ 0.38785116 -0.38785116]\n",
      " [ 0.27816033 -0.27816033]\n",
      " [ 0.17293131 -0.17293131]\n",
      " [ 0.19229347 -0.19229347]\n",
      " [ 0.49584252 -0.49584252]\n",
      " [ 0.2639251  -0.2639251 ]\n",
      " [ 0.14515091 -0.14515091]\n",
      " [ 0.23620734 -0.23620734]\n",
      " [ 0.2634591  -0.2634591 ]\n",
      " [ 0.23060539 -0.23060539]\n",
      " [ 0.36803116 -0.36803116]\n",
      " [ 0.24625037 -0.24625037]\n",
      " [ 0.0353612  -0.0353612 ]\n",
      " [ 0.23553927 -0.23553927]\n",
      " [ 0.2517206  -0.2517206 ]\n",
      " [ 0.24456924 -0.24456924]\n",
      " [ 0.45539558 -0.45539558]\n",
      " [ 0.38730859 -0.38730859]\n",
      " [ 0.4988643  -0.4988643 ]\n",
      " [ 0.16709877 -0.16709877]\n",
      " [ 0.4629206  -0.4629206 ]\n",
      " [ 0.23721086 -0.23721086]\n",
      " [ 0.44386142 -0.44386142]\n",
      " [ 0.33347714 -0.33347714]\n",
      " [ 0.16243584 -0.16243584]\n",
      " [ 0.14362736 -0.14362736]\n",
      " [-0.00725741  0.00725741]\n",
      " [ 0.46099237 -0.46099237]\n",
      " [ 0.2434976  -0.2434976 ]\n",
      " [ 0.37214275 -0.37214275]\n",
      " [ 0.19449032 -0.19449032]\n",
      " [ 0.21977494 -0.21977494]\n",
      " [ 0.26079989 -0.26079989]\n",
      " [ 0.16153339 -0.16153339]\n",
      " [ 0.27314799 -0.27314799]\n",
      " [ 0.35653047 -0.35653047]\n",
      " [ 0.24739517 -0.24739517]]\n",
      "0.12655288576705317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# step 3. evaluate the model\n",
    "alpha = torch.matmul(X_train, alpha_matrix)\n",
    "mu = torch.matmul(X_train, mu_matrix)\n",
    "sd = torch.abs(torch.matmul(torch.abs(X_train), sd_matrix))\n",
    "pi = torch.softmax(alpha, dim=1)\n",
    "\n",
    "# calculate the loss\n",
    "loss = WDL(y_train, q_vec, mu, sd, pi)\n",
    "# calculate the difference between estimators and real values\n",
    "print((mu - mu_true).detach().numpy())\n",
    "print((sd - sd_true).detach().numpy())\n",
    "print((pi - pi_true).detach().numpy())\n",
    "print(loss.item())\n",
    "print()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "201b5660",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1710302979.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Displaying WDL_example.txt.\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# I forgot its purpose\n",
    "\"\"\"optimizer.param_groups[0]['lr'] = 0.5\n",
    "for k in range(500):\n",
    "    alpha = torch.matmul(X_train[:, 2:3], alpha_matrix)\n",
    "    mu = torch.matmul(X_train[:, [0, 4, 6]], mu_matrix)\n",
    "    sd = torch.abs(torch.matmul(torch.abs(X_train[:, [0, 1, 6]]), sd_matrix))\n",
    "\n",
    "    alpha = torch.matmul(X_train, alpha_matrix)\n",
    "    mu = torch.matmul(X_train, mu_matrix)\n",
    "    sd = torch.abs(torch.matmul(torch.abs(X_train), sd_matrix))\n",
    "\n",
    "    pi = torch.softmax(alpha, dim=1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = WDL(y_train, q_vec, mu, sd, pi)\n",
    "    print(k, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"alpha = torch.matmul(X_train[:, 2:3], alpha_matrix)\n",
    "mu = torch.matmul(X_train[:, [0, 4, 6]], mu_matrix)\n",
    "sd = torch.abs(torch.matmul(torch.abs(X_train[:, [0, 1, 6]]), sd_matrix))\"\"\"\n",
    "# WDL_example.txt\n",
    "# Displaying WDL_example.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7738a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
